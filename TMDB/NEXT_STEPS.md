# ä¸‹ä¸€æ­¥å„ªåŒ–æŒ‡å—

## ğŸ¯ ç•¶å‰ç‹€æ…‹
- **æœ€ä½³æ¨¡å‹**: Gradient Boosting
- **RÂ² åˆ†æ•¸**: 0.7034
- **RMSE**: $75,770,328
- **ç‰¹å¾µæ•¸é‡**: 83å€‹

## ğŸš€ ç«‹å³å¯åšï¼ˆ5åˆ†é˜å…§ï¼‰

### 1. æäº¤åˆ° Kaggle
```bash
# ä½¿ç”¨é€²éšæ¨¡å‹çš„é æ¸¬çµæœ
# æª”æ¡ˆ: submission_advanced.csv
```

### 2. å®‰è£é€²éšæ¨¡å‹åº«
```bash
pip install xgboost lightgbm catboost
```

ç„¶å¾Œé‡æ–°åŸ·è¡Œï¼š
```bash
python advanced_model.py
```

**é æœŸæ”¹é€²**:
- RÂ² æå‡è‡³ 0.75-0.78
- RMSE é™ä½è‡³ $60-65M
- Kaggle æ’åæå‡ 20-30%

---

## ğŸ“ˆ çŸ­æœŸå„ªåŒ–ï¼ˆ1-2å°æ™‚ï¼‰

### 1. è¶…åƒæ•¸èª¿å„ª

å‰µå»º `hyperparameter_tuning.py`:

```python
from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import GradientBoostingRegressor

# å®šç¾©åƒæ•¸ç©ºé–“
param_dist = {
    'n_estimators': [200, 300, 400, 500],
    'max_depth': [5, 6, 7, 8, 9],
    'learning_rate': [0.01, 0.02, 0.03, 0.05],
    'subsample': [0.7, 0.8, 0.9],
    'min_samples_split': [2, 3, 4],
    'min_samples_leaf': [1, 2, 3]
}

# éš¨æ©Ÿæœç´¢
random_search = RandomizedSearchCV(
    GradientBoostingRegressor(random_state=42),
    param_distributions=param_dist,
    n_iter=50,
    cv=3,
    scoring='r2',
    n_jobs=-1,
    verbose=2
)

random_search.fit(X_train, y_train)
print(f"æœ€ä½³åƒæ•¸: {random_search.best_params_}")
print(f"æœ€ä½³åˆ†æ•¸: {random_search.best_score_}")
```

**é æœŸæ”¹é€²**: RÂ² +0.01-0.02

### 2. 5-Fold äº¤å‰é©—è­‰

ä¿®æ”¹ `advanced_model.py` æ·»åŠ ï¼š

```python
from sklearn.model_selection import cross_val_score

# 5-Fold CV
cv_scores = cross_val_score(
    model, X, y_log, 
    cv=5, 
    scoring='r2',
    n_jobs=-1
)

print(f"CV RÂ² åˆ†æ•¸: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})")
```

**é æœŸæ”¹é€²**: æ›´ç©©å®šçš„è©•ä¼°ï¼Œæ¸›å°‘éæ“¬åˆ

### 3. ç‰¹å¾µé¸æ“‡

```python
from sklearn.feature_selection import SelectFromModel

# ä½¿ç”¨ç‰¹å¾µé‡è¦æ€§é¸æ“‡
selector = SelectFromModel(
    GradientBoostingRegressor(n_estimators=100),
    threshold='median'
)
selector.fit(X_train, y_train)

# ç²å–é¸ä¸­çš„ç‰¹å¾µ
selected_features = X.columns[selector.get_support()]
print(f"é¸ä¸­ {len(selected_features)} å€‹ç‰¹å¾µ")
```

**é æœŸæ”¹é€²**: æ¸›å°‘éæ“¬åˆï¼Œæå‡æ³›åŒ–èƒ½åŠ›

---

## ğŸ¨ ä¸­æœŸå„ªåŒ–ï¼ˆ1å¤©ï¼‰

### 1. Stacking é›†æˆ

```python
from sklearn.ensemble import StackingRegressor

# åŸºç¤æ¨¡å‹
estimators = [
    ('rf', RandomForestRegressor(...)),
    ('gb', GradientBoostingRegressor(...)),
    ('xgb', xgb.XGBRegressor(...)),
    ('lgb', lgb.LGBMRegressor(...))
]

# å…ƒæ¨¡å‹
stacking = StackingRegressor(
    estimators=estimators,
    final_estimator=Ridge(alpha=10),
    cv=5
)

stacking.fit(X_train, y_train)
```

**é æœŸæ”¹é€²**: RÂ² +0.02-0.03

### 2. æ›´å¤šç‰¹å¾µå·¥ç¨‹

#### A. æ–‡æœ¬ç‰¹å¾µï¼ˆTF-IDFï¼‰
```python
from sklearn.feature_extraction.text import TfidfVectorizer

# Overview çš„ TF-IDF
tfidf = TfidfVectorizer(max_features=50, stop_words='english')
overview_tfidf = tfidf.fit_transform(df['overview'].fillna(''))
```

#### B. æ™‚é–“åºåˆ—ç‰¹å¾µ
```python
# åŒå¹´ä¸Šæ˜ é›»å½±æ•¸é‡
df['movies_same_year'] = df.groupby('release_year')['id'].transform('count')

# åŒæœˆä¸Šæ˜ é›»å½±å¹³å‡ç¥¨æˆ¿
df['avg_revenue_same_month'] = df.groupby('release_month')['revenue'].transform('mean')
```

#### C. ç¶²çµ¡ç‰¹å¾µ
```python
# æ¼”å“¡åˆä½œæ¬¡æ•¸
actor_pairs = {}
for cast_list in df['cast_list']:
    for i in range(len(cast_list)):
        for j in range(i+1, len(cast_list)):
            pair = tuple(sorted([cast_list[i]['name'], cast_list[j]['name']]))
            actor_pairs[pair] = actor_pairs.get(pair, 0) + 1
```

**é æœŸæ”¹é€²**: RÂ² +0.01-0.02

### 3. ç•°å¸¸å€¼è™•ç†

```python
# è­˜åˆ¥ç•°å¸¸å€¼
Q1 = df['revenue'].quantile(0.25)
Q3 = df['revenue'].quantile(0.75)
IQR = Q3 - Q1

# ç§»é™¤æ¥µç«¯ç•°å¸¸å€¼ï¼ˆå¯é¸ï¼‰
df_clean = df[
    (df['revenue'] >= Q1 - 3*IQR) & 
    (df['revenue'] <= Q3 + 3*IQR)
]

# æˆ–ä½¿ç”¨ Robust Scaler
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
```

**é æœŸæ”¹é€²**: æå‡ç©©å®šæ€§

---

## ğŸ”¬ é•·æœŸå„ªåŒ–ï¼ˆ1é€±+ï¼‰

### 1. å¤–éƒ¨æ•¸æ“šå¢å¼·

#### IMDb è©•åˆ†
```python
# éœ€è¦çˆ¬èŸ²æˆ– API
df['imdb_rating'] = df['imdb_id'].apply(get_imdb_rating)
df['imdb_votes'] = df['imdb_id'].apply(get_imdb_votes)
```

#### Rotten Tomatoes
```python
df['rt_score'] = df['title'].apply(get_rt_score)
df['rt_audience_score'] = df['title'].apply(get_rt_audience)
```

#### ç¤¾äº¤åª’é«”
```python
# Twitter/Facebook æåŠæ¬¡æ•¸
df['social_mentions'] = df['title'].apply(get_social_mentions)
```

**é æœŸæ”¹é€²**: RÂ² +0.03-0.05

### 2. æ·±åº¦å­¸ç¿’æ¨¡å‹

```python
import tensorflow as tf
from tensorflow import keras

# æ§‹å»ºç¥ç¶“ç¶²çµ¡
model = keras.Sequential([
    keras.layers.Dense(256, activation='relu', input_shape=(n_features,)),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dense(1)
])

model.compile(
    optimizer='adam',
    loss='mse',
    metrics=['mae']
)

model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)
```

**é æœŸæ”¹é€²**: å¯èƒ½ +0.02-0.04ï¼ˆéœ€è¦å¤§é‡èª¿å„ªï¼‰

### 3. AutoML

```python
# ä½¿ç”¨ H2O AutoML
import h2o
from h2o.automl import H2OAutoML

h2o.init()
train_h2o = h2o.H2OFrame(train_df)

aml = H2OAutoML(max_runtime_secs=3600, seed=42)
aml.train(x=feature_cols, y='revenue', training_frame=train_h2o)

# ç²å–æœ€ä½³æ¨¡å‹
best_model = aml.leader
```

**é æœŸæ”¹é€²**: è‡ªå‹•æ‰¾åˆ°æœ€ä½³æ¨¡å‹çµ„åˆ

---

## ğŸ“Š è©•ä¼°æ”¹é€²æ•ˆæœ

### å‰µå»ºè©•ä¼°è…³æœ¬

```python
# evaluate_improvements.py

import pandas as pd
import numpy as np
from sklearn.metrics import mean_squared_error, r2_score

def evaluate_submission(submission_file, validation_file):
    """è©•ä¼°æäº¤æª”æ¡ˆ"""
    sub = pd.read_csv(submission_file)
    val = pd.read_csv(validation_file)
    
    merged = pd.merge(sub, val, on='id')
    
    rmse = np.sqrt(mean_squared_error(merged['revenue_true'], merged['revenue_pred']))
    r2 = r2_score(merged['revenue_true'], merged['revenue_pred'])
    
    print(f"RMSE: ${rmse:,.0f}")
    print(f"RÂ²: {r2:.4f}")
    
    return rmse, r2

# æ¯”è¼ƒä¸åŒç‰ˆæœ¬
versions = {
    'V1.0 åŸºç¤': 'submission.csv',
    'V2.0 é€²éš': 'submission_advanced.csv',
    'V3.0 å„ªåŒ–': 'submission_optimized.csv'
}

for name, file in versions.items():
    print(f"\n{name}:")
    evaluate_submission(file, 'validation.csv')
```

---

## ğŸ¯ ç›®æ¨™è¨­å®š

### çŸ­æœŸç›®æ¨™ï¼ˆæœ¬é€±ï¼‰
- [ ] RÂ² > 0.75
- [ ] RMSE < $70M
- [ ] Kaggle æ’åé€²å…¥å‰ 50%

### ä¸­æœŸç›®æ¨™ï¼ˆæœ¬æœˆï¼‰
- [ ] RÂ² > 0.78
- [ ] RMSE < $65M
- [ ] Kaggle æ’åé€²å…¥å‰ 30%

### é•·æœŸç›®æ¨™ï¼ˆæœ¬å­£ï¼‰
- [ ] RÂ² > 0.80
- [ ] RMSE < $60M
- [ ] Kaggle æ’åé€²å…¥å‰ 10%

---

## ğŸ“ å¯¦é©—è¿½è¹¤

å‰µå»º `experiments.csv` è¿½è¹¤æ‰€æœ‰å¯¦é©—ï¼š

| æ—¥æœŸ | ç‰ˆæœ¬ | æ¨¡å‹ | ç‰¹å¾µæ•¸ | RÂ² | RMSE | å‚™è¨» |
|------|------|------|--------|----|----|------|
| 2025-12-09 | V1.0 | RF | 14 | 0.688 | $72.4M | åŸºç¤ç‰ˆæœ¬ |
| 2025-12-09 | V2.0 | GB | 83 | 0.703 | $75.8M | æ­·å²ç‰¹å¾µ |
| 2025-12-09 | V2.0 | Ensemble | 83 | 0.698 | $76.5M | 3æ¨¡å‹é›†æˆ |
| ... | ... | ... | ... | ... | ... | ... |

---

## ğŸ”§ èª¿è©¦æŠ€å·§

### 1. æª¢æŸ¥é æ¸¬åˆ†å¸ƒ
```python
import matplotlib.pyplot as plt

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.hist(y_true, bins=50, alpha=0.7, label='True')
plt.hist(y_pred, bins=50, alpha=0.7, label='Predicted')
plt.legend()
plt.title('Distribution Comparison')

plt.subplot(1, 3, 2)
plt.scatter(y_true, y_pred, alpha=0.3)
plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')
plt.xlabel('True Revenue')
plt.ylabel('Predicted Revenue')
plt.title('Prediction vs True')

plt.subplot(1, 3, 3)
residuals = y_true - y_pred
plt.scatter(y_pred, residuals, alpha=0.3)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel('Predicted Revenue')
plt.ylabel('Residuals')
plt.title('Residual Plot')

plt.tight_layout()
plt.savefig('prediction_analysis.png')
```

### 2. ç‰¹å¾µç›¸é—œæ€§åˆ†æ
```python
import seaborn as sns

# è¨ˆç®—ç›¸é—œæ€§
corr_matrix = df[top_features + ['revenue']].corr()

# è¦–è¦ºåŒ–
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Feature Correlation Matrix')
plt.savefig('feature_correlation.png')
```

### 3. å­¸ç¿’æ›²ç·š
```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, val_scores = learning_curve(
    model, X, y, 
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5, 
    scoring='r2'
)

plt.plot(train_sizes, train_scores.mean(axis=1), label='Training')
plt.plot(train_sizes, val_scores.mean(axis=1), label='Validation')
plt.xlabel('Training Size')
plt.ylabel('RÂ² Score')
plt.legend()
plt.title('Learning Curve')
plt.savefig('learning_curve.png')
```

---

## ğŸ’¡ å°ˆå®¶å»ºè­°

1. **ä¸è¦éåº¦æ“¬åˆè¨“ç·´é›†**
   - å§‹çµ‚é—œæ³¨é©—è­‰é›†æ€§èƒ½
   - ä½¿ç”¨äº¤å‰é©—è­‰
   - æ—©åœï¼ˆEarly Stoppingï¼‰

2. **ç‰¹å¾µå·¥ç¨‹ > æ¨¡å‹é¸æ“‡**
   - å¥½çš„ç‰¹å¾µæ¯”è¤‡é›œçš„æ¨¡å‹æ›´é‡è¦
   - å°ˆæ³¨æ–¼é ˜åŸŸçŸ¥è­˜
   - å‰µé€ æœ‰æ„ç¾©çš„äº¤äº’ç‰¹å¾µ

3. **é›†æˆå­¸ç¿’æ˜¯ç‹é“**
   - å¤šæ¨£åŒ–çš„æ¨¡å‹çµ„åˆ
   - ä¸åŒé¡å‹çš„æ¨¡å‹ï¼ˆæ¨¹æ¨¡å‹ + ç·šæ€§æ¨¡å‹ï¼‰
   - é©ç•¶çš„æ¬Šé‡åˆ†é…

4. **æŒçºŒè¿­ä»£**
   - å°æ­¥å¿«è·‘
   - è¨˜éŒ„æ¯æ¬¡å¯¦é©—
   - å¾å¤±æ•—ä¸­å­¸ç¿’

---

## ğŸ“š å­¸ç¿’è³‡æº

- [Kaggle Learn - Feature Engineering](https://www.kaggle.com/learn/feature-engineering)
- [Kaggle Learn - Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [LightGBM Documentation](https://lightgbm.readthedocs.io/)
- [Ensemble Methods Guide](https://scikit-learn.org/stable/modules/ensemble.html)

---

**è¨˜ä½**: æ©Ÿå™¨å­¸ç¿’æ˜¯ä¸€å€‹è¿­ä»£éç¨‹ã€‚æ¯æ¬¡æ”¹é€²éƒ½æ˜¯é€²æ­¥ï¼ğŸš€
